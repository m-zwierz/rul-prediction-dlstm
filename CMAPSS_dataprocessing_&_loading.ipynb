{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Instalacja PyTorch Lightning\n",
        "!pip install pytorch-lightning"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bc7gq__pn6ot",
        "outputId": "09ddf10c-409d-4506-9b73-690ed363a05b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytorch-lightning\n",
            "  Downloading pytorch_lightning-2.6.0-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: torch>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from pytorch-lightning) (2.9.0+cpu)\n",
            "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.12/dist-packages (from pytorch-lightning) (4.67.1)\n",
            "Requirement already satisfied: PyYAML>5.4 in /usr/local/lib/python3.12/dist-packages (from pytorch-lightning) (6.0.3)\n",
            "Requirement already satisfied: fsspec>=2022.5.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (2025.3.0)\n",
            "Collecting torchmetrics>0.7.0 (from pytorch-lightning)\n",
            "  Downloading torchmetrics-1.8.2-py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from pytorch-lightning) (25.0)\n",
            "Requirement already satisfied: typing-extensions>4.5.0 in /usr/local/lib/python3.12/dist-packages (from pytorch-lightning) (4.15.0)\n",
            "Collecting lightning-utilities>=0.10.0 (from pytorch-lightning)\n",
            "  Downloading lightning_utilities-0.15.2-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (3.13.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from lightning-utilities>=0.10.0->pytorch-lightning) (75.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning) (3.20.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning) (3.1.6)\n",
            "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.12/dist-packages (from torchmetrics>0.7.0->pytorch-lightning) (2.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.22.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.1.0->pytorch-lightning) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.1.0->pytorch-lightning) (3.0.3)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.12/dist-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (3.11)\n",
            "Downloading pytorch_lightning-2.6.0-py3-none-any.whl (849 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.5/849.5 kB\u001b[0m \u001b[31m48.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.15.2-py3-none-any.whl (29 kB)\n",
            "Downloading torchmetrics-1.8.2-py3-none-any.whl (983 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m983.2/983.2 kB\u001b[0m \u001b[31m60.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: lightning-utilities, torchmetrics, pytorch-lightning\n",
            "Successfully installed lightning-utilities-0.15.2 pytorch-lightning-2.6.0 torchmetrics-1.8.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. System i Dane\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# 2. PyTorch (Silnik)\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# 3. PyTorch Lightning (Trener)\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "# 4. Preprocessing\n",
        "# To jedyny element z zewnątrz, który zostawiamy, bo ręczne pisanie\n",
        "# normalizacji danych (skalowania do 0-1) to strata czasu i ryzyko błędu.\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Ustawienie ziarna losowości dla powtarzalności\n",
        "pl.seed_everything(42)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A379bRHcn7pe",
        "outputId": "56704917-225e-417c-818c-61ee7eb97e9d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:lightning_fabric.utilities.seed:Seed set to 42\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "42"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/edwardzjl/CMAPSSData.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RCuPUXbXoBXJ",
        "outputId": "f3742ebc-7d5f-4ae3-b87d-eaab80217734"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'CMAPSSData'...\n",
            "remote: Enumerating objects: 16, done.\u001b[K\n",
            "remote: Total 16 (delta 0), reused 0 (delta 0), pack-reused 16 (from 1)\u001b[K\n",
            "Receiving objects: 100% (16/16), 11.96 MiB | 15.44 MiB/s, done.\n",
            "Resolving deltas: 100% (1/1), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "g2DY_PFwnWgE"
      },
      "outputs": [],
      "source": [
        "class CMAPSS_Preprocessor:\n",
        "    def __init__(self, data_path, sequence_length=30, alpha=0.25, max_rul=125):\n",
        "        self.data_path = data_path #ściezka do pliku\n",
        "        self.sequence_length = sequence_length #długość okna czasowego\n",
        "        self.alpha = alpha  # Współczynnik wygładzania z artykułu\n",
        "        self.max_rul = max_rul # Przycinanie RUL (Piecewise Linear)\n",
        "\n",
        "        # Definicja kolumn tablicy na dane\n",
        "        self.index_cols = ['unit_nr', 'time_cycles']\n",
        "        self.setting_cols = ['os_1', 'os_2', 'os_3']\n",
        "        self.sensor_cols = ['s' + str(i) for i in range(1, 22)]\n",
        "        self.cols = self.index_cols + self.setting_cols + self.sensor_cols\n",
        "\n",
        "        # Skaler (będziemy go uczyć tylko na treningu)(?) - normalizacja odczytów z czujników do zakresu 0-1\n",
        "        self.scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "\n",
        "    def process(self, file_name='FD001'):\n",
        "        # 1. Wczytanie danych z  plików do dataframes (tabeli)\n",
        "        train_df = pd.read_csv(f'{self.data_path}/train_{file_name}.txt', sep=r'\\s+', header=None, names=self.cols)\n",
        "        test_df = pd.read_csv(f'{self.data_path}/test_{file_name}.txt', sep=r'\\s+', header=None, names=self.cols)\n",
        "        test_rul_df = pd.read_csv(f'{self.data_path}/RUL_{file_name}.txt', sep=r'\\s+', header=None, names=['RUL'])\n",
        "\n",
        "        # 2. Obliczenie etykiet RUL dla treningu\n",
        "        train_df = self._add_rul(train_df, is_test=False)\n",
        "\n",
        "        # 3. Wygładzanie danych (Exponential Smoothing) - WAŻNE!\n",
        "        train_df = self._smooth_data(train_df)\n",
        "        test_df = self._smooth_data(test_df)\n",
        "\n",
        "        # 4. Normalizacja (Fit na treningu, Transform na obu)\n",
        "        # Bierzemy wszystkie sensory i ustawienia\n",
        "        feats = self.setting_cols + self.sensor_cols #podpisy kolumn    feats = ['os_1', 'os_2', 'os_3', 's1', 's2', ..., 's21']\n",
        "        self.scaler.fit(train_df[feats])      # .fit znajduje max i min w zbiorach feats\n",
        "        train_df[feats] = self.scaler.transform(train_df[feats])  # normalizacja dataframeu treningowego\n",
        "        test_df[feats] = self.scaler.transform(test_df[feats])  # normalizacja dataframeu testowego\n",
        "\n",
        "        # 5. Generowanie okien czasowych (Sliding Window)\n",
        "        X_train, y_train = self._gen_sequence(train_df, feats)\n",
        "        # Uwaga: Dla testu w C-MAPSS zazwyczaj bierze się tylko OSTATNIE okno,\n",
        "        # bo plik RUL_FD001.txt zawiera tylko jedną liczbę dla każdego silnika (RUL na samym końcu).\n",
        "        X_test, y_test = self._gen_test_sequence(test_df, test_rul_df, feats)\n",
        "\n",
        "        return X_train, y_train, X_test, y_test\n",
        "\n",
        "    def _add_rul(self, df, is_test=False):\n",
        "        # Grupowanie po silniku, znalezienie max cyklu\n",
        "        max_life = df.groupby('unit_nr')['time_cycles'].transform('max')\n",
        "        df['RUL'] = max_life - df['time_cycles']\n",
        "\n",
        "        # Implementacja Piecewise Linear RUL (ucinamy powyżej 125)\n",
        "        df['RUL'] = df['RUL'].clip(upper=self.max_rul)\n",
        "        return df\n",
        "\n",
        "    def _smooth_data(self, df):\n",
        "        # WAŻNE: Musimy grupować po 'unit_nr'!\n",
        "        # Nie możemy pozwolić, by wygładzanie przeniosło się z końca Silnika 1 na początek Silnika 2.\n",
        "\n",
        "        df[self.sensor_cols] = df.groupby('unit_nr')[self.sensor_cols].transform(     #wg Gemini .ewm.mean zrobi to samo co wzór na wygładzanie z artykułu\n",
        "            lambda x: x.ewm(alpha=self.alpha, adjust=False).mean()\n",
        "        )\n",
        "        return df\n",
        "\n",
        "    def _gen_sequence(self, df, feature_cols):\n",
        "        X, y = [], []\n",
        "        data_array = df[feature_cols].values    # data_array: Czysta macierz liczb z sensorów i parametrów Wymiar: [20631, 24] -> [Liczba wszystkich cykli, Liczba sensorów]\n",
        "        target_array = df['RUL'].values     # target_array: Wektor RUL. Wymiar: [20631] -> [Liczba wszystkich cykli]\n",
        "        unit_ids = df['unit_nr'].values\n",
        "\n",
        "        # Iterujemy, ale musimy uważać, żeby okno nie \"przeskoczyło\" między silnikami\n",
        "        for i in range(len(df) - self.sequence_length):\n",
        "            # Sprawdzamy czy okno mieści się w JEDNYM silniku\n",
        "            if unit_ids[i] == unit_ids[i + self.sequence_length]: #id silnika takie same na początku i końcu sekwencji\n",
        "                # Dodajemy okno (30 wierszy)\n",
        "                X.append(data_array[i : i + self.sequence_length]) #\n",
        "                # Dodajemy cel (RUL w ostatnim kroku tego okna)\n",
        "                y.append(target_array[i + self.sequence_length - 1])\n",
        "\n",
        "        # np.array(X) ma wymiar: [N, T, K]\n",
        "        # N (Samples) ≈ 17 631 (Wszystkie cykle minus 30 dla każdego ze 100 silników)\n",
        "        # T (Time) = 30 (Długość naszego okna)\n",
        "        # K (Features) = 24 (Liczba sensorów)\n",
        "        # Kształt: (17631, 30, 24)\n",
        "\n",
        "        # np.array(y) ma wymiar: [N]\n",
        "        # Kształt: (17631,) -> Wektor zawierający jedną liczbę RUL dla każdego okna\n",
        "        return np.array(X), np.array(y)\n",
        "\n",
        "    def _gen_test_sequence(self, test_df, truth_df, feature_cols):\n",
        "        X, y = [], []\n",
        "        # Dla zbioru testowego bierzemy tylko OSTATNIE 30 cykli każdego silnika\n",
        "        # I przypisujemy mu prawdziwy RUL z pliku RUL_FD001.txt\n",
        "\n",
        "        true_ruls = truth_df['RUL'].values   #truth_df  Wymiar: [100 wierszy, 1 kolumna] (bo mamy 100 silników testowych i po jednej liczbie RUL dla każdego).\n",
        "\n",
        "        for unit_id in test_df['unit_nr'].unique():     #.unique usuwa wielokrotne id\n",
        "            # Wyciągamy dane jednego silnika\n",
        "            temp_df = test_df[test_df['unit_nr'] == unit_id]\n",
        "\n",
        "            if len(temp_df) >= self.sequence_length:\n",
        "                # Bierzemy ostatnie 30 cykli\n",
        "                window = temp_df[feature_cols].values[-self.sequence_length:]\n",
        "                X.append(window)\n",
        "                # Bierzemy prawdziwy RUL (indeks unit_id-1, bo silniki są od 1, tablica od 0)\n",
        "                y.append(true_ruls[unit_id - 1])\n",
        "\n",
        "        return np.array(X), np.array(y)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CMAPSSDataModule(pl.LightningDataModule):\n",
        "    def __init__(self, data_path='CMAPSSData', batch_size=10, sequence_length=30):\n",
        "        super().__init__()\n",
        "        self.data_path = data_path\n",
        "        self.batch_size = batch_size # Wg artykułu batch=10\n",
        "        self.sequence_length = sequence_length\n",
        "        self.preprocessor = CMAPSS_Preprocessor(data_path, sequence_length)\n",
        "\n",
        "    def setup(self, stage=None):\n",
        "        # Tu dzieje się cała magia przygotowania danych\n",
        "        X_train_np, y_train_np, X_test_np, y_test_np = self.preprocessor.process('FD001')\n",
        "\n",
        "        # Konwersja Numpy -> PyTorch Tensor\n",
        "        self.train_X = torch.tensor(X_train_np, dtype=torch.float32)\n",
        "        self.train_y = torch.tensor(y_train_np, dtype=torch.float32).unsqueeze(1) # [N] -> [N, 1]\n",
        "\n",
        "        self.test_X = torch.tensor(X_test_np, dtype=torch.float32)\n",
        "        self.test_y = torch.tensor(y_test_np, dtype=torch.float32).unsqueeze(1)\n",
        "\n",
        "        print(f\"Dane przygotowane!\")\n",
        "        print(f\"Trening: {self.train_X.shape}\") # Oczekiwane: [17631, 30, 24]\n",
        "        print(f\"Test: {self.test_X.shape}\")     # Oczekiwane: [100, 30, 24]\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        # Shuffle=True jest kluczowe dla treningu!\n",
        "        dataset = torch.utils.data.TensorDataset(self.train_X, self.train_y)\n",
        "        return DataLoader(dataset, batch_size=self.batch_size, shuffle=True)\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        # Używamy zbioru testowego jako walidacji, żeby śledzić postęp (w prawdziwym projekcie można by wydzielić osobny val)\n",
        "        dataset = torch.utils.data.TensorDataset(self.test_X, self.test_y)\n",
        "        return DataLoader(dataset, batch_size=self.batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "leuZttDOnpox"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Inicjalizacja DataModule\n",
        "dm = CMAPSSDataModule(batch_size=10, sequence_length=30)\n",
        "\n",
        "# 2. Uruchomienie setup (wczytanie i przetworzenie)\n",
        "dm.setup()\n",
        "\n",
        "# 3. Sprawdzenie jednej paczki danych\n",
        "# Pobieramy jeden batch z loadera\n",
        "x_batch, y_batch = next(iter(dm.train_dataloader()))\n",
        "\n",
        "print(\"\\n--- Sprawdzenie Batcha ---\")\n",
        "print(f\"Wymiary wejścia (X): {x_batch.shape}\")\n",
        "# Powinno być: torch.Size([10, 30, 24]) -> 10 próbek, 30 cykli, 24 cechy\n",
        "print(f\"Wymiary etykiet (y): {y_batch.shape}\")\n",
        "# Powinno być: torch.Size([10, 1]) -> 10 wyników RUL\n",
        "\n",
        "print(\"\\nPrzykładowy RUL z batcha:\", y_batch[0].item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cCqVDxTSns9C",
        "outputId": "e60be40f-e5bf-4c10-c9cc-76fa11a3ffe1"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dane przygotowane!\n",
            "Trening: torch.Size([17631, 30, 24])\n",
            "Test: torch.Size([100, 30, 24])\n",
            "\n",
            "--- Sprawdzenie Batcha ---\n",
            "Wymiary wejścia (X): torch.Size([10, 30, 24])\n",
            "Wymiary etykiet (y): torch.Size([10, 1])\n",
            "\n",
            "Przykładowy RUL z batcha: 125.0\n"
          ]
        }
      ]
    }
  ]
}